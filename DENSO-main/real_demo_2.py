import cv2
import numpy as np
import torch
from ultralytics import YOLO
import time
import math

from robot_new_2 import RobotArm

# ===================== CONFIG ========================
CAMERA_ID = 0

DICT_TYPE = cv2.aruco.DICT_4X4_1000
ARUCO_WORLD_CM = {
    0:   (0.0, 0.0),      
    100: (23.0, 0.0),     
    110: (23.0, 17.0),    
    10:  (0, 17.0),     
}

# ----- Robot Parameters -----
BOX_DEPTH         = 6.0
HOVER_HEIGHT      = 12.0       
PRE_GRAB_HEIGHT   = 6.5        
GRAB_HEIGHT       = 1.0        
LIFT_HEIGHT       = 10.0       

# T·ªça ƒë·ªô ƒë·∫∑t v·∫≠t sang b√™n ph·∫£i (D·ª±a tr√™n Training Data: X=15, Z=6 -> Base ~41 ƒë·ªô)
PLACE_RIGHT_X     = 25.0
PLACE_RIGHT_Z     = 6.0


# T·ªça ƒë·ªô ƒë·∫∑t v·∫≠t to (ƒë·ªè) sang b√™n ph·∫£i
PLACE_RED_X =  25.0 
PLACE_RED_Z = 6.5
# --- RED placement safety ---
RED_PLACE_HEIGHT = 3.0   # <-- higher than GRAB_HEIGHT to avoid crushing red box



# V·ªã tr√≠ Home (Degrees)
HOME_BASE     = 53    
HOME_SHOULDER = 90    
HOME_ELBOW    = 0     

# K·∫πp 
GRIP_OPEN    = 10
GRIP_CLOSE   = 55              
STEPS_SMOOTH = 20              

MODEL_PATH = "aug_3.pt"
CONF_THRES    = 0.40
CONF_TRIGGER  = 0.65
STABLE_FRAMES = 7
DEVICE = "mps" if torch.cuda.is_available() else "cpu"

#S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c√°c aruco 
def sort_rectangle_points(pts):
    pts = np.array(pts)
    s = pts.sum(axis=1)
    diff = np.diff(pts, axis=1)
    tl = pts[np.argmin(s)]
    br = pts[np.argmax(s)]
    tr = pts[np.argmin(diff)]
    bl = pts[np.argmax(diff)]
    return np.array([tl, tr, br, bl], dtype=np.int32)


#T√¨m t·ªça ƒë·ªô c·ªßa c√°c m√£ aruco 
def compute_homography(aruco_centers, world_map):
    img_pts, world_pts = [], []
    for mid, (X_cm, Z_cm) in world_map.items():
        if mid not in aruco_centers: return None
        cx, cy = aruco_centers[mid]
        img_pts.append([cx, cy])
        world_pts.append([X_cm, Z_cm])
    return cv2.findHomography(np.array(img_pts), np.array(world_pts))[0]

#Chuy·ªÉn t·ªça ƒë·ªô pixel sang v·ªã tr√≠ th·ª±c t·∫ø
def pixel_to_world(H, x_px, y_px):
    pt = np.array([[[x_px, y_px]]], dtype=np.float32)
    out = cv2.perspectiveTransform(pt, H)[0][0]
    return float(out[0]), float(out[1])

# ===================== CONTROL =======================
class PickAndPlaceRobot(RobotArm):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.cur_b = HOME_BASE
        self.cur_s = HOME_SHOULDER
        self.cur_e = HOME_ELBOW
        # 1. V·ªã tr√≠ c√°nh tay ban ƒë·∫ßu (m·ªü tay)
        self.home(GRIP_OPEN)

    def home(self, grip):
        print(f"[ROBOT] V·ªÅ Home (Grip: {grip})")
        # V·ªÅ Home: Base 53, Shoulder 90 (d·ª±ng ƒë·ª©ng), Elbow 0
        self.move_to_angles(HOME_BASE, HOME_SHOULDER, HOME_ELBOW, grip, steps=STEPS_SMOOTH)

    def move_to_angles(self, b, s, e, g, steps=STEPS_SMOOTH):
        if steps <= 1:
            self.send_cmd(b, s, e, g)
            self.cur_b, self.cur_s, self.cur_e = b, s, e
            return
        diff_b = (b - self.cur_b) / steps
        diff_s = (s - self.cur_s) / steps
        diff_e = (e - self.cur_e) / steps
        for i in range(1, steps + 1):
            self.send_cmd(self.cur_b + diff_b * i, self.cur_s + diff_s * i, self.cur_e + diff_e * i, g)
            time.sleep(0.02)
        self.cur_b, self.cur_s, self.cur_e = b, s, e

    def approach_and_pick(self, x, y, z_table):
        """Di chuy·ªÉn ƒë·∫øn v·∫≠t, g·∫Øp v√† nh·∫•c l√™n cao"""
        print(f"[PICK] Di chuy·ªÉn ƒë·∫øn X={x:.1f} Z={y:.1f}")
        
        # 1. Hover (Ti·∫øp c·∫≠n tr√™n cao)
        self.move_to(x, y, HOVER_HEIGHT, g=GRIP_OPEN, steps=STEPS_SMOOTH) 
        time.sleep(0.2)
        
        # 2. H·∫° xu·ªëng v·ªã tr√≠ g·∫Øp
        self.move_to(x, y, GRAB_HEIGHT, g=GRIP_OPEN, steps=STEPS_SMOOTH)
        time.sleep(0.3)
        
        # 3. K·∫πp v·∫≠t
        print("[PICK] K·∫πp v·∫≠t")
        self.send_cmd(self.cur_b, self.cur_s, self.cur_e, GRIP_CLOSE)
        time.sleep(0.8) # ƒê·ª£i k·∫πp ch·∫∑t
        
        # 4. Nh·∫•c l√™n (LIFT_HEIGHT) ƒë·ªÉ tr√°nh va ch·∫°m khi thu v·ªÅ
        print("[PICK] Nh·∫•c v·∫≠t l√™n")
        self.move_to(x, y, LIFT_HEIGHT, g=GRIP_CLOSE, steps=STEPS_SMOOTH)
        time.sleep(0.3)

    def place_sequence_fixed_angle(self):
        """
        Chu tr√¨nh: Home -> Xoay Base 15 -> H·∫° xu·ªëng -> Th·∫£ -> V·ªÅ Home
        """
        # --- C·∫§U H√åNH G√ìC TH·∫¢ V·∫¨T ---
        TARGET_BASE   = 0  # G√≥c quay sang ph·∫£i
        DROP_SHOULDER = 70  # H·∫° vai xu·ªëng (G√≥c c√†ng nh·ªè c√†ng th·∫•p)
        DROP_ELBOW    = 40  # Du·ªói khu·ª∑u ra (Ch·ªânh s·ªë n√†y n·∫øu b·ªã ƒë·∫≠p b√†n ho·∫∑c qu√° cao)
        
        print(f"[PLACE] B·∫Øt ƒë·∫ßu chu tr√¨nh th·∫£ t·∫°i g√≥c Base={TARGET_BASE}")

        # 1. Thu v·ªÅ Home (V·∫´n k·∫πp ch·∫∑t v·∫≠t)
        self.home(GRIP_CLOSE)
        time.sleep(0.5)

        # 2. Xoay Base sang 15 ƒë·ªô (Vai v·∫´n d·ª±ng ƒë·ª©ng 90 ƒë·ªÉ an to√†n)
        self.move_to_angles(TARGET_BASE, HOME_SHOULDER, HOME_ELBOW, GRIP_CLOSE, steps=STEPS_SMOOTH)
        time.sleep(0.5)

        # 3. H·∫° c√°nh tay xu·ªëng v·ªã tr√≠ th·∫£
        # B: 15, S: 45, E: 40
        self.move_to_angles(TARGET_BASE, DROP_SHOULDER, DROP_ELBOW, GRIP_CLOSE, steps=STEPS_SMOOTH)
        time.sleep(0.5)
        
        # 4. M·ªü tay (Th·∫£ v·∫≠t)
        print("[PLACE] Th·∫£ v·∫≠t")
        self.send_cmd(self.cur_b, self.cur_s, self.cur_e, GRIP_OPEN)
        time.sleep(0.5) 
        
        # 5. Nh·∫•c tay l√™n l·∫°i t∆∞ th·∫ø d·ª±ng ƒë·ª©ng (ƒë·ªÉ tr√°nh va qu·∫πt khi quay v·ªÅ)
        # B: 15, S: 90, E: 0
        self.move_to_angles(TARGET_BASE, HOME_SHOULDER, HOME_ELBOW, GRIP_OPEN, steps=STEPS_SMOOTH)
        time.sleep(0.3)

        # 6. Quay l·∫°i Home Init (K·∫øt th√∫c chu tr√¨nh)
        self.home(GRIP_OPEN)
        print("[DONE] ƒê√£ v·ªÅ Home")
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.cur_b = HOME_BASE
        self.cur_s = HOME_SHOULDER
        self.cur_e = HOME_ELBOW
        # 1. V·ªã tr√≠ c√°nh tay ban ƒë·∫ßu (m·ªü tay)
        self.home(GRIP_OPEN)

    def home(self, grip):
        print(f"[ROBOT] V·ªÅ Home (Grip: {grip})")
        self.move_to_angles(HOME_BASE, HOME_SHOULDER, HOME_ELBOW, grip, steps=STEPS_SMOOTH)

    def move_to_angles(self, b, s, e, g, steps=STEPS_SMOOTH):
        if steps <= 1:
            self.send_cmd(b, s, e, g)
            self.cur_b, self.cur_s, self.cur_e = b, s, e
            return
        diff_b = (b - self.cur_b) / steps
        diff_s = (s - self.cur_s) / steps
        diff_e = (e - self.cur_e) / steps
        for i in range(1, steps + 1):
            self.send_cmd(self.cur_b + diff_b * i, self.cur_s + diff_s * i, self.cur_e + diff_e * i, g)
            time.sleep(0.02)
        self.cur_b, self.cur_s, self.cur_e = b, s, e

    def approach_and_pick(self, x, y, z_table):
        # 3. Di chuy·ªÉn ƒë·∫øn v·∫≠t ph·∫©m (m·ªü tay) - Hover -> H·∫° th·∫•p
        print(f"[PICK] Di chuy·ªÉn ƒë·∫øn X={x:.1f} Z={y:.1f}")
        self.move_to(x, y, HOVER_HEIGHT, g=GRIP_OPEN, steps=STEPS_SMOOTH) 
        time.sleep(0.2)
        
        self.move_to(x, y, GRAB_HEIGHT, g=GRIP_OPEN, steps=STEPS_SMOOTH)
        time.sleep(0.3)
        
        # 4. K·∫πp tay
        print("[PICK] K·∫πp v·∫≠t")
        self.send_cmd(self.cur_b, self.cur_s, self.cur_e, GRIP_CLOSE)
        time.sleep(0.8)
        
        # Nh·∫•c l√™n (An to√†n)
        self.move_to(x, y, LIFT_HEIGHT, g=GRIP_CLOSE, steps=STEPS_SMOOTH)
        time.sleep(0.3)

    def place_to_right(self):
        # --- C·∫§U H√åNH G√ìC TH·∫¢ V·∫¨T ---
        TARGET_BASE = 15    # G√≥c Base mong mu·ªën
        DROP_SHOULDER = 25  # G√≥c Vai khi h·∫° xu·ªëng (B·∫°n c√≥ th·ªÉ ch·ªânh s·ªë n√†y ƒë·ªÉ v∆∞∆°n xa/g·∫ßn)
        DROP_ELBOW = 50     # G√≥c Khu·ª∑u khi h·∫° xu·ªëng (B·∫°n c√≥ th·ªÉ ch·ªânh s·ªë n√†y ƒë·ªÉ cao/th·∫•p)
        
        print(f"[PLACE] Xoay Base v·ªÅ g√≥c {TARGET_BASE} ƒë·ªô")

        # 1. Xoay Base v·ªÅ 15 ƒë·ªô (Gi·ªØ Vai v√† Khu·ª∑u ·ªü v·ªã tr√≠ cao an to√†n nh∆∞ Home)
        # HOME_SHOULDER=90, HOME_ELBOW=0 l√† t∆∞ th·∫ø d·ª±ng ƒë·ª©ng tay
        self.move_to_angles(TARGET_BASE, HOME_SHOULDER, HOME_ELBOW, GRIP_CLOSE, steps=STEPS_SMOOTH)
        time.sleep(0.3)

        # 2. H·∫° tay xu·ªëng v·ªã tr√≠ th·∫£ (D√πng g√≥c c·ªë ƒë·ªãnh v√¨ kh√¥ng d√πng t·ªça ƒë·ªô)
        self.move_to_angles(TARGET_BASE, DROP_SHOULDER, DROP_ELBOW, GRIP_CLOSE, steps=STEPS_SMOOTH)
        time.sleep(0.5)

        # 3. M·ªü tay (Th·∫£ v·∫≠t)
        print("[PLACE] Th·∫£ v·∫≠t")
        self.send_cmd(TARGET_BASE, DROP_SHOULDER, DROP_ELBOW, GRIP_OPEN)
        time.sleep(0.5)

        # 4. Nh·∫•c tay l√™n l·∫°i (V·ªÅ t∆∞ th·∫ø an to√†n)
        self.move_to_angles(TARGET_BASE, HOME_SHOULDER, HOME_ELBOW, GRIP_OPEN, steps=STEPS_SMOOTH)
        time.sleep(0.3)
        
        
        
        
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.cur_b = HOME_BASE
        self.cur_s = HOME_SHOULDER
        self.cur_e = HOME_ELBOW
        # 1. V·ªã tr√≠ c√°nh tay ban ƒë·∫ßu (m·ªü tay)
        self.home(GRIP_OPEN)

    def home(self, grip):
        print(f"[ROBOT] V·ªÅ Home (Grip: {grip})")
        self.move_to_angles(HOME_BASE, HOME_SHOULDER, HOME_ELBOW, grip, steps=STEPS_SMOOTH)

    def move_to_angles(self, b, s, e, g, steps=STEPS_SMOOTH):
        if steps <= 1:
            self.send_cmd(b, s, e, g)
            self.cur_b, self.cur_s, self.cur_e = b, s, e
            return
        diff_b = (b - self.cur_b) / steps
        diff_s = (s - self.cur_s) / steps
        diff_e = (e - self.cur_e) / steps
        for i in range(1, steps + 1):
            self.send_cmd(self.cur_b + diff_b * i, self.cur_s + diff_s * i, self.cur_e + diff_e * i, g)
            time.sleep(0.02)
        self.cur_b, self.cur_s, self.cur_e = b, s, e

    def approach_and_pick(self, x, y, z_table):
        # 3. Di chuy·ªÉn ƒë·∫øn v·∫≠t ph·∫©m (m·ªü tay) - Hover -> H·∫° th·∫•p
        print(f"[PICK] Di chuy·ªÉn ƒë·∫øn X={x:.1f} Z={y:.1f}")
        self.move_to(x, y, HOVER_HEIGHT, g=GRIP_OPEN, steps=STEPS_SMOOTH) 
        time.sleep(0.2)
        
        self.move_to(x, y, GRAB_HEIGHT, g=GRIP_OPEN, steps=STEPS_SMOOTH)
        time.sleep(0.3)
        
        # 4. K·∫πp tay
        print("[PICK] K·∫πp v·∫≠t")
        self.send_cmd(self.cur_b, self.cur_s, self.cur_e, GRIP_CLOSE)
        time.sleep(0.8)
        
        # Nh·∫•c l√™n (An to√†n)
        self.home(GRIP_CLOSE)
        time.sleep(0.3)

    def place_to_right(self):
        # 5. Quay sang b√™n ph·∫£i v√† ƒë·∫∑t xu·ªëng (k·∫πp tay)

        print(f"[PLACE] Quay ph·∫£i ƒë·∫øn X={PLACE_RIGHT_X} Z={PLACE_RIGHT_Z}")
        
        # Di chuy·ªÉn sang ph·∫£i ·ªü ƒë·ªô cao an to√†n (Hover)
        self.move_to(PLACE_RIGHT_X, PLACE_RIGHT_Z, HOVER_HEIGHT, g=GRIP_CLOSE, steps=STEPS_SMOOTH)
        time.sleep(0.2)
        
        # H·∫° xu·ªëng
        self.move_to(PLACE_RIGHT_X, PLACE_RIGHT_Z, GRAB_HEIGHT, g=GRIP_CLOSE, steps=STEPS_SMOOTH)
        time.sleep(0.5)
        
        # 6. M·ªü tay
        print("[PLACE] Th·∫£ v·∫≠t")
        self.send_cmd(self.cur_b, self.cur_s, self.cur_e, GRIP_OPEN)
        time.sleep(0.5) 
        
        # Nh·∫•c l√™n l·∫°i ƒë·ªÉ tr√°nh va qu·∫πt
        self.move_to(PLACE_RIGHT_X, PLACE_RIGHT_Z, HOVER_HEIGHT, g=GRIP_OPEN, steps=STEPS_SMOOTH)
       
       
        
    #V·ªã tr√≠ ƒë·∫∑t h·ªôp ƒë·ªè     
    def place_to_red(self):
        print(f"[PLACE] Quay ƒë·∫øn RED X={PLACE_RED_X} Z={PLACE_RED_Z}")

        # Hover
        self.move_to(PLACE_RED_X, PLACE_RED_Z, HOVER_HEIGHT, g=GRIP_CLOSE, steps=STEPS_SMOOTH)
        time.sleep(0.2)

        # Descend (RED uses higher height than normal grab height)
        self.move_to(PLACE_RED_X, PLACE_RED_Z, RED_PLACE_HEIGHT, g=GRIP_CLOSE, steps=STEPS_SMOOTH)
        time.sleep(0.5)

        print("[PLACE] Th·∫£ v·∫≠t (RED)")
        self.send_cmd(self.cur_b, self.cur_s, self.cur_e, GRIP_OPEN)
        time.sleep(0.5)

        # Lift back up
        self.move_to(PLACE_RED_X, PLACE_RED_Z, HOVER_HEIGHT, g=GRIP_OPEN, steps=STEPS_SMOOTH)

    

# ===================== MAIN ==========================
def main():
    cap = cv2.VideoCapture(CAMERA_ID)
    if not cap.isOpened(): raise RuntimeError("‚ùå Cannot open camera")
    
    aruco_dict = cv2.aruco.getPredefinedDictionary(DICT_TYPE)
    aruco_params = cv2.aruco.DetectorParameters()
    detector = cv2.aruco.ArucoDetector(aruco_dict, aruco_params)

    print("[INFO] Loading YOLO...")
    model = YOLO(MODEL_PATH)
    model.to(DEVICE)
    print("[INFO] Connecting robot...")
    robot = PickAndPlaceRobot()          
    
    H = None
    stable_count = 0
    last_pos = None
    # locked_coords = None 
    locked_target = None  # (Xr, Zr, label)


    print("[INFO] Press 'q' to quit | 'r' to UNLOCK coords")

    while True:
        if locked_target is not None:
            Xr, Zr, lbl = locked_target

            print("\n" + "‚àû"*30)
            print(f"üöÄ TH·ª∞C HI·ªÜN CHU·ªñI: X={Xr:.1f} Z={Zr:.1f} | label={lbl}")
            print("‚àû"*30)

            robot.approach_and_pick(Xr, Zr, z_table=Zr + BOX_DEPTH/2)

            # Choose drop position based on label
            if lbl == "yellow":
                robot.place_to_right()   # existing position
            elif lbl == "red":
                robot.place_to_red()     # new placeholder position
            else:
                # fallback: treat unknown as yellow (or you can skip)
                robot.place_to_right()

            robot.home(GRIP_OPEN)

            print("‚úÖ Ho√†n th√†nh chu·ªói. Ch·ªù l·ªánh reset 'r'...")
            time.sleep(3)

            locked_target = None
            print("[RESET] S·∫µn s√†ng g·∫Øp v·∫≠t m·ªõi.")

            # Ch·ªù ng∆∞·ªùi d√πng b·∫•m r ƒë·ªÉ g·∫Øp v·∫≠t ti·∫øp theo
            # while True:
            #     k = cv2.waitKey(10) & 0xFF
            #     if k == ord('q'): 
            #         cap.release()
            #         cv2.destroyAllWindows()
            #         robot.close()
            #         return
            #     if k == ord('r'): 
            #         locked_coords = None
            #         print("[RESET] S·∫µn s√†ng g·∫Øp v·∫≠t m·ªõi.")
            #         break
            # continue 

        ret, frame = cap.read()
        if not ret: break

        overlay = frame.copy()
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        corners, ids, _ = detector.detectMarkers(gray)
        aruco_centers = {}
        if ids is not None:
            for i, mid in enumerate(ids.flatten()):
                pts = corners[i][0]
                cx, cy = int(pts[:,0].mean()), int(pts[:,1].mean())
                aruco_centers[mid] = (cx, cy)

        if all(mid in aruco_centers for mid in ARUCO_WORLD_CM):
            rect_pts = sort_rectangle_points([aruco_centers[mid] for mid in ARUCO_WORLD_CM])
            cv2.polylines(frame, [rect_pts], True, (0,200,0), 2)
            H_new = compute_homography(aruco_centers, ARUCO_WORLD_CM)
            if H_new is not None: H = H_new

        if H is not None:
            mask = np.zeros(frame.shape[:2], np.uint8)
            cv2.fillPoly(mask, [rect_pts], 255)
            masked = cv2.bitwise_and(frame, frame, mask=mask)
            results = model(masked, conf=CONF_THRES, device=DEVICE, verbose=False)


            for r in results:
                for box in r.boxes:
                    x1,y1,x2,y2 = map(int, box.xyxy[0])
                    conf = float(box.conf)
                    cls_id = int(box.cls)  # <-- add
                    lbl = model.names.get(cls_id, str(cls_id))  # <-- add (expects "red"/"yellow")

                    cx, cy = (x1 + x2) // 2, (y1 + y2) // 2
                    Xw, Zw = pixel_to_world(H, cx, cy)
                    Xr = Xw
                    Zr = Zw - BOX_DEPTH / 2 + 1.2

                    cv2.rectangle(frame, (x1,y1), (x2,y2), (0,0,255), 2)
                    cv2.putText(frame, f"{lbl} X={Xr:.1f} Z={Zr:.1f}", (x1, y2+20),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)

                    if conf >= CONF_TRIGGER:
                        if last_pos and abs(Xr - last_pos[0]) < 0.6 and abs(Zr - last_pos[1]) < 0.6:
                            stable_count += 1
                        else:
                            stable_count = 1
                            last_pos = (Xr, Zr)

                        cv2.putText(frame, f"Stable {stable_count}/{STABLE_FRAMES}",
                                    (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 2)

                        if stable_count >= STABLE_FRAMES:
                            locked_target = (Xr, Zr, lbl)
                            print(f"\nLOCKED: X={Xr:.1f} Z={Zr:.1f} | label={lbl}")


        cv2.imshow("Vision Pipeline", frame)
        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'): break
        if key == ord('r'):
            stable_count = 0
            last_pos = None
            locked_target = None

        

    cap.release()
    cv2.destroyAllWindows()
    robot.close()

if __name__ == "__main__":
    main()